{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# stgan_kaggle_friendly.py\n# Kaggle-friendly STGAN-lite: two-stage GAN with Freeze-D + Barlow Twins (SSL)\n# - Lightweight conv generator/discriminator (fits Kaggle GPUs)\n# - Stage-1: train unconditional GAN on all images (128x128)\n# - Stage-2: for each class, fine-tune from Stage-1 with Freeze-D + Barlow Twins self-supervised loss\n# - Generate synthetic images, combine with reals, train T-ResNet50 classifier (TTA optional)\n# Usage: run as a script in Kaggle notebook or terminal\n\nimport os, shutil, random, math, time\nfrom pathlib import Path\nimport argparse\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as T\nfrom torchvision.datasets import ImageFolder\nimport torchvision.models as models\nfrom torchvision.utils import save_image, make_grid\n\n# ---------------------------\n# Config (edit for your environment)\n# ---------------------------\nDATA_DIR = os.getenv('DATA_DIR', '/kaggle/input/skin-cancer-mnist-ham10000')  # original files\nWORK_DIR = os.getenv('WORK_DIR', '/kaggle/working/stgan_lite')\nREAL_IMGS_DIR = os.path.join(WORK_DIR, 'real_images')  # per-class folders will be created here\nSTAGE1_DIR = os.path.join(WORK_DIR, 'stage1')\nSTAGE2_DIR = os.path.join(WORK_DIR, 'stage2')\nSYNTH_DIR = os.path.join(WORK_DIR, 'synth')\nCOMBINED_DIR = os.path.join(WORK_DIR, 'combined')\nCLASS_NAMES = [\"akiec\",\"bcc\",\"bkl\",\"df\",\"mel\",\"nv\",\"vasc\"]\nIMG_SIZE = 128   # small to be Kaggle-friendly\nBATCH = 32\nLATENT_DIM = 128\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42\ntorch.manual_seed(SEED); random.seed(SEED); np.random.seed(SEED)\nos.makedirs(WORK_DIR, exist_ok=True)\n\n# ---------------------------\n# Helpers: organize HAM10000 into per-class folders if needed\n# ---------------------------\ndef organize_ham10000(data_dir, out_dir):\n    # expects HAM10000 metadata CSV and image files inside data_dir\n    import csv\n    meta = Path(data_dir) / \"HAM10000_metadata.csv\"\n    if not meta.exists():\n        print(\"Metadata CSV not found in\", data_dir)\n        return\n    outp = Path(out_dir)\n    outp.mkdir(parents=True, exist_ok=True)\n    for c in CLASS_NAMES:\n        (outp/c).mkdir(exist_ok=True)\n    # find images (jpg/png)\n    for img_path in Path(data_dir).glob(\"**/*\"):\n        if img_path.suffix.lower() in ['.jpg','.jpeg','.png']:\n            stem = img_path.stem\n            # read metadata mapping\n            # we'll parse CSV once\n    # load mapping\n    mapping = {}\n    with open(meta, 'r') as f:\n        reader = csv.DictReader(f)\n        for r in reader:\n            mapping[r['image_id']] = r['dx'].lower()\n    # copy\n    for img_path in Path(data_dir).glob(\"**/*\"):\n        if img_path.suffix.lower() in ['.jpg','.jpeg','.png']:\n            id = img_path.stem\n            if id in mapping and mapping[id] in CLASS_NAMES:\n                dst = Path(out_dir)/mapping[id]/img_path.name\n                if not dst.exists():\n                    shutil.copy(img_path, dst)\n    print(\"Organized images under\", out_dir)\n\n# ---------------------------\n# Dataset\n# ---------------------------\nclass ImgFolderDataset(Dataset):\n    def __init__(self, folder, transform=None):\n        self.paths = list(Path(folder).rglob(\"*.jpg\")) + list(Path(folder).rglob(\"*.png\"))\n        self.transform = transform\n    def __len__(self): return len(self.paths)\n    def __getitem__(self, idx):\n        img = Image.open(self.paths[idx]).convert(\"RGB\")\n        if self.transform: img = self.transform(img)\n        return img\n\n# ---------------------------\n# Small generator & discriminator (GAN-lite)\n# ---------------------------\ndef conv_block(in_ch, out_ch, kernel=3, stride=1, padding=1, activation=True):\n    layers = [nn.Conv2d(in_ch, out_ch, kernel, stride, padding)]\n    layers.append(nn.BatchNorm2d(out_ch))\n    if activation: layers.append(nn.LeakyReLU(0.2, inplace=True))\n    return nn.Sequential(*layers)\n\nclass SimpleGenerator(nn.Module):\n    def __init__(self, latent_dim=128, out_channels=3, fmap=64, img_size=128):\n        super().__init__()\n        self.init_size = img_size // 16  # we will upsample 4x (x2 x2 x2 x2)\n        self.l1 = nn.Linear(latent_dim, fmap*8 * self.init_size * self.init_size)\n        self.conv_blocks = nn.Sequential(\n            conv_block(fmap*8, fmap*8),\n            nn.Upsample(scale_factor=2),\n            conv_block(fmap*8, fmap*4),\n            nn.Upsample(scale_factor=2),\n            conv_block(fmap*4, fmap*2),\n            nn.Upsample(scale_factor=2),\n            conv_block(fmap*2, fmap),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(fmap, out_channels, 3, 1, 1),\n            nn.Tanh()\n        )\n    def forward(self, z):\n        out = self.l1(z).view(z.size(0), -1, self.init_size, self.init_size)\n        return self.conv_blocks(out)\n\nclass SimpleDiscriminator(nn.Module):\n    def __init__(self, in_channels=3, fmap=64):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels, fmap, 4, 2, 1),\n            nn.LeakyReLU(0.2, inplace=True),\n            conv_block(fmap, fmap*2),\n            nn.AvgPool2d(2),\n            conv_block(fmap*2, fmap*4),\n            nn.AvgPool2d(2),\n            conv_block(fmap*4, fmap*8),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.fc = nn.Linear(fmap*8, 1)\n        # small projector for Barlow Twins (optional)\n        self.projector = nn.Sequential(nn.Linear(fmap*8, 256), nn.ReLU(), nn.Linear(256,128))\n\n    def forward(self, x, return_feat=False):\n        f = self.model(x).view(x.size(0), -1)\n        logits = self.fc(f)\n        if return_feat:\n            return logits.view(-1,1), f, self.projector(f)\n        return logits.view(-1,1)\n\n# ---------------------------\n# Barlow Twins loss (simple)\n# ---------------------------\ndef barlow_twins_loss(z_a, z_b, lam_offdiag=0.005):\n    # z_a, z_b: [B, D] already projected; normalize feature dims\n    B, D = z_a.size()\n    # normalize batchwise per-dim\n    za = (z_a - z_a.mean(0)) / (z_a.std(0) + 1e-9)\n    zb = (z_b - z_b.mean(0)) / (z_b.std(0) + 1e-9)\n    C = (za.T @ zb) / B  # D x D\n    on_diag = torch.diagonal(C).add_(-1).pow(2).sum()\n    off_diag = (C.pow(2).sum() - torch.diagonal(C).pow(2).sum())\n    return on_diag + lam_offdiag * off_diag\n\n# ---------------------------\n# Augmentations\n# ---------------------------\ndef get_gan_transforms(img_size):\n    tf = T.Compose([\n        T.Resize((img_size, img_size)),\n        T.RandomHorizontalFlip(),\n        T.RandomRotation(10),\n        T.ColorJitter(0.1,0.1,0.1,0.05),\n        T.ToTensor(),\n        T.Normalize([0.5]*3, [0.5]*3)\n    ])\n    return tf\n\n# ---------------------------\n# Training utilities\n# ---------------------------\nbce_loss = nn.BCEWithLogitsLoss()\n\ndef train_gan_stage1(generator, discriminator, dataloader, epochs=6, lr=2e-4, save_dir=STAGE1_DIR):\n    device = DEVICE\n    G, D = generator.to(device), discriminator.to(device)\n    optG = optim.Adam(G.parameters(), lr=lr, betas=(0.5,0.999))\n    optD = optim.Adam(D.parameters(), lr=lr, betas=(0.5,0.999))\n    os.makedirs(save_dir, exist_ok=True)\n    for epoch in range(epochs):\n        G.train(); D.train()\n        for real in dataloader:\n            real = real.to(device)\n            bs = real.size(0)\n            # Discriminator step\n            z = torch.randn(bs, LATENT_DIM, device=device)\n            fake = G(z)\n            logits_real = D(real)\n            logits_fake = D(fake.detach())\n            lossD = bce_loss(logits_real, torch.ones_like(logits_real)) + bce_loss(logits_fake, torch.zeros_like(logits_fake))\n            optD.zero_grad(); lossD.backward(); optD.step()\n            # Generator step\n            logits_fake2 = D(fake)\n            lossG = bce_loss(logits_fake2, torch.ones_like(logits_fake2))\n            optG.zero_grad(); lossG.backward(); optG.step()\n        print(f\"[Stage1] Epoch {epoch+1}/{epochs} | D_loss: {lossD.item():.4f} | G_loss: {lossG.item():.4f}\")\n        # save snapshots\n        torch.save(G.state_dict(), os.path.join(save_dir, f\"G_epoch{epoch+1}.pth\"))\n        torch.save(D.state_dict(), os.path.join(save_dir, f\"D_epoch{epoch+1}.pth\"))\n    return G, D\n\ndef freeze_discriminator_top_layers(D: nn.Module, num_top=1):\n    # heuristic: freeze last N layers of the sequential blocks (we inspect named_parameters order)\n    params = list(D.named_parameters())\n    if num_top <= 0: return\n    # freeze last 20% parameters per 'num_top' unit; simpler: freeze last num_top parameter groups\n    for name, p in params[-(num_top*3):]:\n        p.requires_grad = False\n\ndef train_gan_stage2_per_class(class_name, class_folder, G_init, D_init, epochs=4, lr=1e-4,\n                               freeze_d_num_layers=2, barlow_lambda=1.0, batch=BATCH, save_root=STAGE2_DIR):\n    \"\"\"\n    Fine-tune from global G_init/D_init on class_folder.\n    Adds Freeze-D and Barlow Twins SSL (on real images in batch).\n    \"\"\"\n    os.makedirs(save_root, exist_ok=True)\n    # dataset/dataloader\n    tf = get_gan_transforms(IMG_SIZE)\n    ds = ImgFolderDataset(class_folder, transform=tf)\n    if len(ds) < 4:\n        print(f\"Class {class_name} has {len(ds)} images; skipping stage2 fine-tune (too few).\")\n        return None, None\n    dl = DataLoader(ds, batch_size=batch, shuffle=True, drop_last=True)\n    # models\n    G = SimpleGenerator(LATENT_DIM, img_size=IMG_SIZE).to(DEVICE)\n    D = SimpleDiscriminator().to(DEVICE)\n    G.load_state_dict(torch.load(G_init, map_location=DEVICE))\n    D.load_state_dict(torch.load(D_init, map_location=DEVICE))\n    optG = optim.Adam(G.parameters(), lr=lr, betas=(0.5,0.999))\n    optD = optim.Adam([p for p in D.parameters() if p.requires_grad], lr=lr*0.5, betas=(0.5,0.999))\n    # Freeze-D\n    freeze_discriminator_top_layers(D, freeze_d_num_layers)\n    print(f\"[Stage2:{class_name}] freeze top layers applied; trainable params:\", sum(p.requires_grad for p in D.parameters()))\n    for epoch in range(epochs):\n        G.train(); D.train()\n        for real in dl:\n            real = real.to(DEVICE); bs = real.size(0)\n            # Discriminator standard GAN loss\n            z = torch.randn(bs, LATENT_DIM, device=DEVICE)\n            fake = G(z)\n            logits_real, _, _ = D(real, return_feat=True)\n            logits_fake, _, _ = D(fake.detach(), return_feat=True)\n            lossD = bce_loss(logits_real, torch.ones_like(logits_real)) + bce_loss(logits_fake, torch.zeros_like(logits_fake))\n            # Barlow Twins SSL on real images (two augmented views)\n            # create two augmentations: using light jitter transforms\n            aug = T.Compose([T.Resize((IMG_SIZE,IMG_SIZE)), T.RandomHorizontalFlip(), T.ColorJitter(0.1,0.1,0.1,0.05), T.ToTensor(), T.Normalize([0.5]*3,[0.5]*3)])\n            # build two views by reloading original PIL images\n            # NOTE: our real is already a transformed tensor; for simplicity we jitter tensor by small transforms:\n            # create view_a/view_b by small pixel noise + flip\n            view_a = real * (1 + 0.02*torch.randn_like(real))\n            view_b = real.flip(-1) if random.random() < 0.5 else real * (1 + 0.02*torch.randn_like(real))\n            _, feat_a, proj_a = D(view_a, return_feat=True)\n            _, feat_b, proj_b = D(view_b, return_feat=True)\n            # compute barlow twins on projectors\n            L_bt = barlow_twins_loss(proj_a, proj_b)\n            lossD_total = lossD + barlow_lambda * L_bt\n            optD.zero_grad(); lossD_total.backward(); optD.step()\n            # Generator step\n            z2 = torch.randn(bs, LATENT_DIM, device=DEVICE)\n            fake2 = G(z2)\n            logits_fake2 = D(fake2)[0] if isinstance(D(fake2), tuple) else D(fake2)\n            lossG = bce_loss(logits_fake2, torch.ones_like(logits_fake2))\n            optG.zero_grad(); lossG.backward(); optG.step()\n        print(f\"[Stage2:{class_name}] Epoch {epoch+1}/{epochs} | D_loss: {lossD.item():.4f} | BT_loss: {L_bt.item():.4f} | G_loss: {lossG.item():.4f}\")\n        # save checkpoint per epoch\n        torch.save(G.state_dict(), os.path.join(save_root, f\"{class_name}_G_epoch{epoch+1}.pth\"))\n        torch.save(D.state_dict(), os.path.join(save_root, f\"{class_name}_D_epoch{epoch+1}.pth\"))\n    return G, D\n\n# ---------------------------\n# Synthesis helper\n# ---------------------------\ndef generate_synthetic_images(G_checkpoint, out_folder, n_images=200, truncation=0.7):\n    os.makedirs(out_folder, exist_ok=True)\n    G = SimpleGenerator(LATENT_DIM, img_size=IMG_SIZE).to(DEVICE)\n    G.load_state_dict(torch.load(G_checkpoint, map_location=DEVICE))\n    G.eval()\n    batch = 16\n    idx = 0\n    with torch.no_grad():\n        for start in range(0, n_images, batch):\n            bs = min(batch, n_images - start)\n            z = torch.randn(bs, LATENT_DIM, device=DEVICE)\n            fake = G(z)\n            fake = (fake + 1) / 2.0  # to [0,1]\n            for i in range(bs):\n                save_image(fake[i], os.path.join(out_folder, f\"{idx:05d}.png\"))\n                idx += 1\n    print(\"Saved\", idx, \"images to\", out_folder)\n\n# ---------------------------\n# Combine real + synth and train T-ResNet50 classifier\n# ---------------------------\nclass TResNet50(nn.Module):\n    def __init__(self, num_classes=len(CLASS_NAMES)):\n        super().__init__()\n        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        in_f = resnet.fc.in_features\n        resnet.fc = nn.Sequential(nn.Linear(in_f,128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128,num_classes))\n        self.model = resnet\n    def forward(self,x): return self.model(x)\n\ndef assemble_combined(real_dir, synth_root, out_dir):\n    os.makedirs(out_dir, exist_ok=True)\n    for cls in CLASS_NAMES:\n        src_real = Path(real_dir)/cls\n        t = Path(out_dir)/cls\n        t.mkdir(parents=True, exist_ok=True)\n        if src_real.exists():\n            for f in src_real.glob('*'):\n                shutil.copy(f, t/f.name)\n        synth = Path(synth_root)/cls\n        if synth.exists():\n            for f in synth.glob('*'):\n                shutil.copy(f, t/f.name)\n    print(\"Combined dataset at\", out_dir)\n\ndef train_classifier(combined_dir, epochs=6, bs=32, lr=1e-4):\n    train_tf = T.Compose([T.Resize((224,224)), T.RandomHorizontalFlip(), T.ToTensor(), T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n    val_tf = T.Compose([T.Resize((224,224)), T.ToTensor(), T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n    ds = ImageFolder(combined_dir, transform=train_tf)\n    total = len(ds)\n    train_sz = int(0.8*total); val_sz = int(0.1*total); test_sz = total - train_sz - val_sz\n    train_ds, val_ds, test_ds = random_split(ds, [train_sz,val_sz,test_sz])\n    val_ds.dataset.transform = val_tf; test_ds.dataset.transform = val_tf\n    tr = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=2)\n    vl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=2)\n    model = TResNet50().to(DEVICE)\n    opt = optim.Adam(model.parameters(), lr=lr)\n    crit = nn.CrossEntropyLoss()\n    best = 0.0\n    for e in range(epochs):\n        model.train(); running_loss=0; corr=0; tot=0\n        for imgs, labels in tr:\n            imgs,labels = imgs.to(DEVICE), labels.to(DEVICE)\n            opt.zero_grad(); outs = model(imgs); loss = crit(outs, labels); loss.backward(); opt.step()\n            running_loss += loss.item()*imgs.size(0)\n            preds = outs.argmax(1); corr += (preds==labels).sum().item(); tot += labels.size(0)\n        train_acc = corr/tot\n        # val\n        model.eval(); vcorr=0; vtot=0\n        with torch.no_grad():\n            for imgs, labels in vl:\n                imgs,labels = imgs.to(DEVICE), labels.to(DEVICE)\n                outs = model(imgs); preds = outs.argmax(1)\n                vcorr += (preds==labels).sum().item(); vtot += labels.size(0)\n        vacc = vcorr/vtot\n        print(f\"[Classifier] Epoch {e+1}/{epochs} train_acc={train_acc:.4f} val_acc={vacc:.4f}\")\n        if vacc > best:\n            best = vacc; torch.save(model.state_dict(), os.path.join(WORK_DIR, 't_resnet_best.pth'))\n    print(\"Classifier training finished. best val acc:\", best)\n    return os.path.join(WORK_DIR, 't_resnet_best.pth')\n\n# ---------------------------\n# Orchestration / main\n# ---------------------------\ndef main_pipeline():\n    # 0) organize HAM10000 into per-class folders if not already\n    if not Path(REAL_IMGS_DIR).exists() or not any(Path(REAL_IMGS_DIR).iterdir()):\n        print(\"Organizing HAM10000 into per-class folders...\")\n        organize_ham10000(DATA_DIR, REAL_IMGS_DIR)\n\n    # 1) Create Stage1 dataset (all images) and dataloader\n    tf = get_gan_transforms(IMG_SIZE)\n    ds_all = ImgFolderDataset(REAL_IMGS_DIR, transform=tf)\n    dl_all = DataLoader(ds_all, batch_size=BATCH, shuffle=True, drop_last=True)\n    print(\"Stage1 dataset size:\", len(ds_all))\n\n    # 2) instantiate model and train Stage1 (lightweight)\n    G = SimpleGenerator(LATENT_DIM, img_size=IMG_SIZE)\n    D = SimpleDiscriminator()\n    G, D = train_gan_stage1(G, D, dl_all, epochs=4, lr=2e-4, save_dir=STAGE1_DIR)\n\n    # save latest checkpoints\n    # choose last saved epoch files\n    lastG = sorted(Path(STAGE1_DIR).glob(\"G_epoch*.pth\"))[-1]\n    lastD = sorted(Path(STAGE1_DIR).glob(\"D_epoch*.pth\"))[-1]\n\n    # 3) Stage-2 per-class fine-tune (Freeze-D + Barlow)\n    for cls in CLASS_NAMES:\n        class_folder = os.path.join(REAL_IMGS_DIR, cls)\n        if not Path(class_folder).exists() or len(list(Path(class_folder).glob('*'))) < 10:\n            print(f\"Skipping Stage-2 for {cls}: too few images.\")\n            continue\n        outroot = os.path.join(STAGE2_DIR, cls)\n        os.makedirs(outroot, exist_ok=True)\n        # fine-tune\n        Gc, Dc = train_gan_stage2_per_class(cls, class_folder, str(lastG), str(lastD),\n                                            epochs=3, lr=1e-4, freeze_d_num_layers=2, barlow_lambda=0.5,\n                                            batch=min(BATCH, max(4,len(list(Path(class_folder).glob('*'))))))\n        # if fine-tuning succeeded, generate synthetic\n        if Gc is not None:\n            # use final saved file\n            chk = sorted(Path(outroot).glob(f\"{cls}_G_epoch*.pth\"))\n            if chk:\n                generate_synthetic_images(str(chk[-1]), os.path.join(SYNTH_DIR, cls), n_images=200)\n\n    # 4) For classes where stage2 didn't run, optionally generate from stage1 GAN (to keep balance)\n    for cls in CLASS_NAMES:\n        sdir = Path(SYNTH_DIR)/cls\n        if not sdir.exists() or len(list(sdir.glob('*'))) < 100:\n            print(f\"Generating fallback synth for {cls} from stage1 generator\")\n            generate_synthetic_images(str(lastG), os.path.join(SYNTH_DIR, cls), n_images=200)\n\n    # 5) Combine and train classifier\n    assemble_combined(REAL_IMGS_DIR, SYNTH_DIR, COMBINED_DIR)\n    ckpt = train_classifier(COMBINED_DIR, epochs=6, bs=32, lr=1e-4)\n    print(\"Pipeline complete. classifier ckpt:\", ckpt)\n\nif __name__ == \"__main__\":\n    main_pipeline()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:03:14.095252Z","iopub.execute_input":"2025-11-30T14:03:14.095558Z","iopub.status.idle":"2025-11-30T14:27:51.092306Z","shell.execute_reply.started":"2025-11-30T14:03:14.095531Z","shell.execute_reply":"2025-11-30T14:27:51.091540Z"}},"outputs":[{"name":"stdout","text":"Organizing HAM10000 into per-class folders...\nOrganized images under /kaggle/working/stgan_lite/real_images\nStage1 dataset size: 10015\n[Stage1] Epoch 1/4 | D_loss: 0.6897 | G_loss: 1.8895\n[Stage1] Epoch 2/4 | D_loss: 0.8267 | G_loss: 1.8509\n[Stage1] Epoch 3/4 | D_loss: 1.0526 | G_loss: 1.1082\n[Stage1] Epoch 4/4 | D_loss: 0.2145 | G_loss: 1.9753\n[Stage2:akiec] freeze top layers applied; trainable params: 14\n[Stage2:akiec] Epoch 1/3 | D_loss: 0.5193 | BT_loss: 7.8907 | G_loss: 1.4636\n[Stage2:akiec] Epoch 2/3 | D_loss: 0.6158 | BT_loss: 7.6159 | G_loss: 1.1701\n[Stage2:akiec] Epoch 3/3 | D_loss: 0.5248 | BT_loss: 5.9850 | G_loss: 1.5373\n[Stage2:bcc] freeze top layers applied; trainable params: 14\n[Stage2:bcc] Epoch 1/3 | D_loss: 0.4821 | BT_loss: 6.7404 | G_loss: 1.6699\n[Stage2:bcc] Epoch 2/3 | D_loss: 0.3657 | BT_loss: 6.3744 | G_loss: 1.8138\n[Stage2:bcc] Epoch 3/3 | D_loss: 0.4785 | BT_loss: 5.7535 | G_loss: 1.2733\n[Stage2:bkl] freeze top layers applied; trainable params: 14\n[Stage2:bkl] Epoch 1/3 | D_loss: 0.6501 | BT_loss: 5.7059 | G_loss: 1.5613\n[Stage2:bkl] Epoch 2/3 | D_loss: 0.6521 | BT_loss: 5.5239 | G_loss: 1.3215\n[Stage2:bkl] Epoch 3/3 | D_loss: 0.7697 | BT_loss: 5.4411 | G_loss: 0.9449\n[Stage2:df] freeze top layers applied; trainable params: 14\n[Stage2:df] Epoch 1/3 | D_loss: 0.3360 | BT_loss: 10.7001 | G_loss: 1.9264\n[Stage2:df] Epoch 2/3 | D_loss: 0.3077 | BT_loss: 9.6681 | G_loss: 1.9466\n[Stage2:df] Epoch 3/3 | D_loss: 0.3338 | BT_loss: 7.8674 | G_loss: 2.1095\n[Stage2:mel] freeze top layers applied; trainable params: 14\n[Stage2:mel] Epoch 1/3 | D_loss: 0.4894 | BT_loss: 5.7245 | G_loss: 1.7780\n[Stage2:mel] Epoch 2/3 | D_loss: 0.5794 | BT_loss: 5.4486 | G_loss: 1.3658\n[Stage2:mel] Epoch 3/3 | D_loss: 0.7708 | BT_loss: 5.2010 | G_loss: 1.2123\n[Stage2:nv] freeze top layers applied; trainable params: 14\n[Stage2:nv] Epoch 1/3 | D_loss: 0.7716 | BT_loss: 4.4856 | G_loss: 1.2802\n[Stage2:nv] Epoch 2/3 | D_loss: 0.5975 | BT_loss: 4.1452 | G_loss: 1.3732\n[Stage2:nv] Epoch 3/3 | D_loss: 0.6900 | BT_loss: 3.4788 | G_loss: 1.3954\n[Stage2:vasc] freeze top layers applied; trainable params: 14\n[Stage2:vasc] Epoch 1/3 | D_loss: 0.3179 | BT_loss: 9.3128 | G_loss: 2.1970\n[Stage2:vasc] Epoch 2/3 | D_loss: 0.3375 | BT_loss: 8.5087 | G_loss: 2.0105\n[Stage2:vasc] Epoch 3/3 | D_loss: 0.3221 | BT_loss: 7.2094 | G_loss: 1.8467\nGenerating fallback synth for akiec from stage1 generator\nSaved 200 images to /kaggle/working/stgan_lite/synth/akiec\nGenerating fallback synth for bcc from stage1 generator\nSaved 200 images to /kaggle/working/stgan_lite/synth/bcc\nGenerating fallback synth for bkl from stage1 generator\nSaved 200 images to /kaggle/working/stgan_lite/synth/bkl\nGenerating fallback synth for df from stage1 generator\nSaved 200 images to /kaggle/working/stgan_lite/synth/df\nGenerating fallback synth for mel from stage1 generator\nSaved 200 images to /kaggle/working/stgan_lite/synth/mel\nGenerating fallback synth for nv from stage1 generator\nSaved 200 images to /kaggle/working/stgan_lite/synth/nv\nGenerating fallback synth for vasc from stage1 generator\nSaved 200 images to /kaggle/working/stgan_lite/synth/vasc\nCombined dataset at /kaggle/working/stgan_lite/combined\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 219MB/s]\n","output_type":"stream"},{"name":"stdout","text":"[Classifier] Epoch 1/6 train_acc=0.6424 val_acc=0.7327\n[Classifier] Epoch 2/6 train_acc=0.7616 val_acc=0.7520\n[Classifier] Epoch 3/6 train_acc=0.8192 val_acc=0.7415\n[Classifier] Epoch 4/6 train_acc=0.8611 val_acc=0.7572\n[Classifier] Epoch 5/6 train_acc=0.8777 val_acc=0.7634\n[Classifier] Epoch 6/6 train_acc=0.8872 val_acc=0.7677\nClassifier training finished. best val acc: 0.7677475898334793\nPipeline complete. classifier ckpt: /kaggle/working/stgan_lite/t_resnet_best.pth\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.metrics import (\n    confusion_matrix, classification_report, roc_auc_score,\n    roc_curve, auc, precision_recall_curve\n)\nimport itertools\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:27:51.093944Z","iopub.execute_input":"2025-11-30T14:27:51.094518Z","iopub.status.idle":"2025-11-30T14:27:51.766446Z","shell.execute_reply.started":"2025-11-30T14:27:51.094465Z","shell.execute_reply":"2025-11-30T14:27:51.765692Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ==========================================\n# VISUALIZATION & METRIC UTILITIES\n# ==========================================\n\ndef show_random_real_images(folder_path, class_names, n=4):\n    plt.figure(figsize=(10, 6))\n    for i, cls in enumerate(class_names):\n        img_paths = list((Path(folder_path) / cls).glob(\"*\"))\n        for j in range(n):\n            plt.subplot(len(class_names), n, i*n + j + 1)\n            img = Image.open(random.choice(img_paths))\n            plt.imshow(img)\n            plt.axis(\"off\")\n            if j == 0:\n                plt.ylabel(cls, fontsize=12)\n    plt.suptitle(\"Random Real Images Per Class\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_random_synthetic_images(folder_path, class_names, n=4):\n    plt.figure(figsize=(10, 6))\n    for i, cls in enumerate(class_names):\n        img_paths = list((Path(folder_path) / cls).glob(\"*\"))\n        for j in range(n):\n            plt.subplot(len(class_names), n, i*n + j + 1)\n            img = Image.open(random.choice(img_paths))\n            plt.imshow(img)\n            plt.axis(\"off\")\n            if j == 0:\n                plt.ylabel(cls, fontsize=12)\n    plt.suptitle(\"Random Synthetic Images Per Class\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_confusion_matrix(cm, classes):\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, annot=True, fmt='d',\n                xticklabels=classes,\n                yticklabels=classes, cmap=\"Blues\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n\ndef plot_roc_curves(y_true, y_pred_proba, classes):\n    plt.figure(figsize=(8,6))\n    for i, cls in enumerate(classes):\n        fpr, tpr, _ = roc_curve((y_true == i).astype(int), y_pred_proba[:, i])\n        auc_score = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc_score:.3f})\")\n    plt.plot([0,1],[0,1],'--',color='gray')\n    plt.title(\"ROC Curves\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend()\n    plt.show()\n\n\ndef plot_precision_recall(y_true, y_pred_proba, classes):\n    plt.figure(figsize=(8,6))\n    for i, cls in enumerate(classes):\n        pr, rc, _ = precision_recall_curve((y_true == i).astype(int), y_pred_proba[:, i])\n        plt.plot(rc, pr, label=cls)\n    plt.title(\"Precision–Recall Curves\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.legend()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:28:17.183076Z","iopub.execute_input":"2025-11-30T14:28:17.183376Z","iopub.status.idle":"2025-11-30T14:28:17.194023Z","shell.execute_reply.started":"2025-11-30T14:28:17.183353Z","shell.execute_reply":"2025-11-30T14:28:17.193337Z"}},"outputs":[],"execution_count":4}]}