{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0 — Installs + imports\n!pip install timm --quiet\n\nimport os, random, math, sys\nfrom pathlib import Path\nfrom pprint import pprint\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image, make_grid\nimport torchvision.models as models\nimport timm\n\n# Device\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device:\", DEVICE)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:36:55.224671Z","iopub.execute_input":"2025-11-29T23:36:55.225333Z","iopub.status.idle":"2025-11-29T23:37:03.670655Z","shell.execute_reply.started":"2025-11-29T23:36:55.225305Z","shell.execute_reply":"2025-11-29T23:37:03.669750Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 1 — Find dataset root & metadata automatically (robust)\n# The common Kaggle HAM10000 dataset folder names vary; try a few.\npossible_roots = [\n    \"/kaggle/input/ham10000\",\n    \"/kaggle/input/skin-cancer-mnist-ham10000\",\n    \"/kaggle/input/hab10000\",  # fallback\n]\nROOT_CANDIDATES = [Path(p) for p in possible_roots if Path(p).exists()]\nif len(ROOT_CANDIDATES) == 0:\n    # try any folder under /kaggle/input\n    root = Path(\"/kaggle/input\")\n    subs = list(root.iterdir()) if root.exists() else []\n    if len(subs)>0:\n        ROOT_CANDIDATES = [subs[0]]\n        print(\"Auto selected:\", subs[0])\n    else:\n        raise RuntimeError(\"Couldn't find /kaggle/input dataset. Place HAM10000 in Kaggle Input.\")\nDATA_ROOT = ROOT_CANDIDATES[0]\nprint(\"Using dataset root:\", DATA_ROOT)\n\n# try to locate image folder and metadata csv\n# common layout: HAM10000_images/*.jpg and HAM10000_metadata.csv OR skin-cancer-mnist-ham10000/images/*.jpg and labels.csv\nIMAGE_DIR = None\nMETA_CSV = None\n# search patterns\nfor p in DATA_ROOT.rglob(\"*\"):\n    if p.is_dir() and any(p.glob(\"*.jpg\")):\n        IMAGE_DIR = p\n        break\n# find metadata\nfor fname in [\"HAM10000_metadata.csv\",\"metadata.csv\",\"labels.csv\",\"HAM10000_images_metadata.csv\"]:\n    candidate = DATA_ROOT / fname\n    if candidate.exists():\n        META_CSV = candidate\n        break\n# fallback: find first csv\nif META_CSV is None:\n    csvs = list(DATA_ROOT.rglob(\"*.csv\"))\n    META_CSV = csvs[0] if csvs else None\n\nprint(\"Image dir:\", IMAGE_DIR)\nprint(\"Metadata csv:\", META_CSV)\nif IMAGE_DIR is None:\n    raise RuntimeError(\"No image folder (.jpg/.png) found under dataset root.\")\nif META_CSV is None:\n    print(\"Warning: metadata CSV not found. We'll try to infer class labels from filenames if possible.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:03.672200Z","iopub.execute_input":"2025-11-29T23:37:03.672605Z","iopub.status.idle":"2025-11-29T23:37:03.689612Z","shell.execute_reply.started":"2025-11-29T23:37:03.672578Z","shell.execute_reply":"2025-11-29T23:37:03.688902Z"}},"outputs":[{"name":"stdout","text":"Using dataset root: /kaggle/input/skin-cancer-mnist-ham10000\nImage dir: /kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\nMetadata csv: /kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2 — Build metadata dataframe (robustly)\n# Preferred: HAM10000 metadata with columns 'image_id' and 'dx' (paper uses 'dx' for diagnosis label)\nif META_CSV is not None:\n    df_meta = pd.read_csv(META_CSV)\n    print(\"Loaded metadata columns:\", df_meta.columns.tolist())\n    # Try to standardize\n    if 'image_id' in df_meta.columns and 'dx' in df_meta.columns:\n        df_meta = df_meta[['image_id','dx']].copy()\n    else:\n        # try alternate names\n        if 'lesion_id' in df_meta.columns and 'dx' in df_meta.columns:\n            df_meta = df_meta[['lesion_id','dx']].rename(columns={'lesion_id':'image_id'})\n        else:\n            # attempt to map filename -> label if there is a column containing 'image' and one containing 'label'\n            col_img = None\n            col_lbl = None\n            for c in df_meta.columns:\n                if 'image' in c.lower():\n                    col_img = c\n                if any(x in c.lower() for x in ['dx','label','diagnosis','class']):\n                    col_lbl = c\n            if col_img and col_lbl:\n                df_meta = df_meta[[col_img,col_lbl]].rename(columns={col_img:'image_id', col_lbl:'dx'})\n            else:\n                print(\"Could not find image_id/dx in metadata; showing head for inspection:\")\n                display(df_meta.head())\n                # we'll not error here; fallback to filename scanning later\nelse:\n    df_meta = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:03.690677Z","iopub.execute_input":"2025-11-29T23:37:03.690915Z","iopub.status.idle":"2025-11-29T23:37:03.723605Z","shell.execute_reply.started":"2025-11-29T23:37:03.690889Z","shell.execute_reply":"2025-11-29T23:37:03.722796Z"}},"outputs":[{"name":"stdout","text":"Loaded metadata columns: ['lesion_id', 'image_id', 'dx', 'dx_type', 'age', 'sex', 'localization']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 2 — Build metadata dataframe (robustly)\n# Preferred: HAM10000 metadata with columns 'image_id' and 'dx' (paper uses 'dx' for diagnosis label)\nif META_CSV is not None:\n    df_meta = pd.read_csv(META_CSV)\n    print(\"Loaded metadata columns:\", df_meta.columns.tolist())\n    # Try to standardize\n    if 'image_id' in df_meta.columns and 'dx' in df_meta.columns:\n        df_meta = df_meta[['image_id','dx']].copy()\n    else:\n        # try alternate names\n        if 'lesion_id' in df_meta.columns and 'dx' in df_meta.columns:\n            df_meta = df_meta[['lesion_id','dx']].rename(columns={'lesion_id':'image_id'})\n        else:\n            # attempt to map filename -> label if there is a column containing 'image' and one containing 'label'\n            col_img = None\n            col_lbl = None\n            for c in df_meta.columns:\n                if 'image' in c.lower():\n                    col_img = c\n                if any(x in c.lower() for x in ['dx','label','diagnosis','class']):\n                    col_lbl = c\n            if col_img and col_lbl:\n                df_meta = df_meta[[col_img,col_lbl]].rename(columns={col_img:'image_id', col_lbl:'dx'})\n            else:\n                print(\"Could not find image_id/dx in metadata; showing head for inspection:\")\n                display(df_meta.head())\n                # we'll not error here; fallback to filename scanning later\nelse:\n    df_meta = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:03.725624Z","iopub.execute_input":"2025-11-29T23:37:03.725886Z","iopub.status.idle":"2025-11-29T23:37:03.751418Z","shell.execute_reply.started":"2025-11-29T23:37:03.725866Z","shell.execute_reply":"2025-11-29T23:37:03.750858Z"}},"outputs":[{"name":"stdout","text":"Loaded metadata columns: ['lesion_id', 'image_id', 'dx', 'dx_type', 'age', 'sex', 'localization']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 3 — Dataset + transforms\nIMG_SIZE_GAN = 64         # GAN image size for speed (paper used 256 but 64/128 speeds up)\nIMG_SIZE_CLS = 224        # classifier input\nBATCH_SIZE = 32\nNUM_WORKERS = 2\n\n# GAN transforms (normalize to [-1,1])\ngan_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE_GAN, IMG_SIZE_GAN)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3),\n])\n\n# For Barlow Twins we need two augmented views: define a small augmentation pipeline\nbt_aug = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE_GAN, scale=(0.8,1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3),\n])\n\n# classifier transforms\ncls_train_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE_CLS, IMG_SIZE_CLS)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\ncls_val_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE_CLS, IMG_SIZE_CLS)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:03.752322Z","iopub.execute_input":"2025-11-29T23:37:03.752595Z","iopub.status.idle":"2025-11-29T23:37:03.760679Z","shell.execute_reply.started":"2025-11-29T23:37:03.752576Z","shell.execute_reply":"2025-11-29T23:37:03.759992Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cell 4 — Utility: build per-class path lists using metadata if available\nfrom collections import defaultdict\n\ndef build_class_index(image_dir: Path, meta_df: pd.DataFrame = None):\n    \"\"\"\n    Returns: dict label -> list(paths)\n    \"\"\"\n    class_paths = defaultdict(list)\n    if meta_df is not None and 'image_id' in meta_df.columns and 'dx' in meta_df.columns:\n        # metadata image_id may be w/o extension. try common extensions\n        for _, row in meta_df.iterrows():\n            img_id = str(row['image_id'])\n            lbl = str(row['dx'])\n            found = None\n            for ext in ['.jpg','.png','.jpeg','.JPG']:\n                p = image_dir / (img_id + ext)\n                if p.exists():\n                    found = p\n                    break\n            if found:\n                class_paths[lbl].append(str(found))\n    # fallback: scan filenames and attempt to match label tokens (not reliable but fallback)\n    if len(class_paths)==0:\n        # map using folder names or filename tokens\n        for p in image_dir.rglob(\"*\"):\n            if p.suffix.lower() in (\".jpg\",\".png\",\".jpeg\"):\n                name = p.name.lower()\n                # try to detect common HAM10000 labels inside filenames (mel, nv, bkl, etc.)\n                tokens = ['nv','mel','bkl','akiec','bcc','vasc','df']\n                matched = None\n                for t in tokens:\n                    if t in name:\n                        matched = t\n                        break\n                if matched:\n                    class_paths[matched].append(str(p))\n                else:\n                    # place in 'unknown'\n                    class_paths['unknown'].append(str(p))\n    return class_paths\n\nclass_index = build_class_index(IMAGE_DIR, df_meta)\nfor k in list(class_index.keys()):\n    print(k, len(class_index[k]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:03.762343Z","iopub.execute_input":"2025-11-29T23:37:03.762589Z","iopub.status.idle":"2025-11-29T23:37:15.010293Z","shell.execute_reply.started":"2025-11-29T23:37:03.762568Z","shell.execute_reply":"2025-11-29T23:37:15.009447Z"}},"outputs":[{"name":"stdout","text":"bkl 564\nnv 3431\ndf 56\nmel 435\nvasc 65\nbcc 266\nakiec 183\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 5 — Dataset classes used for Stage-2 fine-tuning\n# Determine 7 classes if present, else use keys from class_index\nCLS_NAMES = sorted(list(class_index.keys()))\nprint(\"Detected classes:\", CLS_NAMES)\n# If metadata has the official labels order, keep that. If not, we'll use detected keys.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:15.011185Z","iopub.execute_input":"2025-11-29T23:37:15.011618Z","iopub.status.idle":"2025-11-29T23:37:15.015485Z","shell.execute_reply.started":"2025-11-29T23:37:15.011590Z","shell.execute_reply":"2025-11-29T23:37:15.014709Z"}},"outputs":[{"name":"stdout","text":"Detected classes: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 6 — PyTorch datasets for GAN and per-class use\nclass SimpleImageDataset(Dataset):\n    def __init__(self, paths, transform=None):\n        self.paths = paths\n        self.transform = transform\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        img = Image.open(p).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img\n\n# full dataset for Stage-1\nall_image_paths = []\nfor v in class_index.values():\n    all_image_paths += v\nall_image_paths = sorted(set(all_image_paths))\nprint(\"Total unique images found:\", len(all_image_paths))\nfull_dataset = SimpleImageDataset(all_image_paths, transform=gan_transform)\nfull_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:15.016198Z","iopub.execute_input":"2025-11-29T23:37:15.016497Z","iopub.status.idle":"2025-11-29T23:37:15.040307Z","shell.execute_reply.started":"2025-11-29T23:37:15.016469Z","shell.execute_reply":"2025-11-29T23:37:15.039391Z"}},"outputs":[{"name":"stdout","text":"Total unique images found: 5000\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 7 — Simple DCGAN-like generator & discriminator (paper used StyleGAN2, but for Kaggle speed we use lightweight model)\nLATENT_DIM = 100\n\nclass Gen(nn.Module):\n    def __init__(self, latent_dim=LATENT_DIM, ngf=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, ngf*8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf*8), nn.ReLU(True),\n            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf*4), nn.ReLU(True),\n            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf*2), nn.ReLU(True),\n            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf), nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n    def forward(self, z):\n        return self.net(z)\n\nclass Disc(nn.Module):\n    def __init__(self, ndf=64, return_features=False):\n        super().__init__()\n        self.return_features = return_features\n        self.conv1 = nn.Sequential(nn.Conv2d(3, ndf, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True))\n        self.conv2 = nn.Sequential(nn.Conv2d(ndf, ndf*2, 4, 2, 1), nn.BatchNorm2d(ndf*2), nn.LeakyReLU(0.2, inplace=True))\n        self.conv3 = nn.Sequential(nn.Conv2d(ndf*2, ndf*4, 4, 2, 1), nn.BatchNorm2d(ndf*4), nn.LeakyReLU(0.2, inplace=True))\n        self.conv4 = nn.Sequential(nn.Conv2d(ndf*4, ndf*8, 4, 2, 1), nn.BatchNorm2d(ndf*8), nn.LeakyReLU(0.2, inplace=True))\n        self.head = nn.Conv2d(ndf*8, 1, 4, 1, 0)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        feat = self.conv4(x)   # penultimate features (spatial)\n        out = self.head(feat)\n        out = out.view(-1,1)\n        if self.return_features:\n            # global-average pool features into vector for Barlow Twins\n            feat_vec = torch.flatten(torch.mean(feat, dim=[2,3]), 1)\n            return out, feat_vec\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:15.041887Z","iopub.execute_input":"2025-11-29T23:37:15.042180Z","iopub.status.idle":"2025-11-29T23:37:15.056162Z","shell.execute_reply.started":"2025-11-29T23:37:15.042150Z","shell.execute_reply":"2025-11-29T23:37:15.055476Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 8 — Instantiate models + optimizers\nG = Gen().to(DEVICE)\nD = Disc(return_features=True).to(DEVICE)\n\ncriterion_bce = nn.BCEWithLogitsLoss()   # use logits for stability (we used head w/o sigmoid ideally)\noptim_G = optim.Adam(G.parameters(), lr=2.5e-4, betas=(0.5,0.999))\noptim_D = optim.Adam(D.parameters(), lr=2.5e-4, betas=(0.5,0.999))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:15.058218Z","iopub.execute_input":"2025-11-29T23:37:15.058542Z","iopub.status.idle":"2025-11-29T23:37:15.300920Z","shell.execute_reply.started":"2025-11-29T23:37:15.058522Z","shell.execute_reply":"2025-11-29T23:37:15.299977Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Cell 9 — Helper: save sample grid\nfixed_z = torch.randn(16, LATENT_DIM,1,1, device=DEVICE)\ndef save_sample_grid(epoch, folder=\"/kaggle/working/gan_samples\"):\n    os.makedirs(folder, exist_ok=True)\n    G.eval()\n    with torch.no_grad():\n        imgs = G(fixed_z).cpu()\n    save_image((imgs+1)/2, os.path.join(folder, f\"epoch_{epoch:03d}.png\"), nrow=4)\n    G.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:15.301679Z","iopub.execute_input":"2025-11-29T23:37:15.301912Z","iopub.status.idle":"2025-11-29T23:37:15.316685Z","shell.execute_reply.started":"2025-11-29T23:37:15.301892Z","shell.execute_reply":"2025-11-29T23:37:15.315878Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Cell 10 — Stage-1: Train unconditional GAN (fast-friendly settings)\nEPOCHS_STAGE1 = 20   # paper used longer; reduce for Kaggle demo/time\nprint(\"Stage-1 training (unconditional GAN) ...\")\nfor epoch in range(EPOCHS_STAGE1):\n    running_d = 0.0\n    running_g = 0.0\n    for i, real in enumerate(full_loader):\n        real = real.to(DEVICE)\n        bs = real.size(0)\n        # labels\n        real_label = torch.ones(bs,1, device=DEVICE)\n        fake_label = torch.zeros(bs,1, device=DEVICE)\n\n        # Train D\n        optim_D.zero_grad()\n        out_real, feat_real = D(real)\n        z = torch.randn(bs, LATENT_DIM,1,1, device=DEVICE)\n        fake = G(z)\n        out_fake, feat_fake = D(fake.detach())\n        # BCE with logits: our D returns logits if we used head w/o sigmoid; if not, adjust\n        loss_d_real = criterion_bce(out_real, real_label)\n        loss_d_fake = criterion_bce(out_fake, fake_label)\n        loss_D = (loss_d_real + loss_d_fake) * 0.5\n        loss_D.backward()\n        optim_D.step()\n\n        # Train G\n        optim_G.zero_grad()\n        out_fake_for_g, _ = D(fake)\n        loss_G = criterion_bce(out_fake_for_g, real_label)\n        loss_G.backward()\n        optim_G.step()\n\n        running_d += loss_D.item()\n        running_g += loss_G.item()\n\n    avg_d = running_d / (i+1)\n    avg_g = running_g / (i+1)\n    print(f\"Epoch {epoch+1}/{EPOCHS_STAGE1} | D {avg_d:.4f} | G {avg_g:.4f}\")\n    if (epoch+1) % 5 == 0:\n        save_sample_grid(epoch+1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:37:15.317676Z","iopub.execute_input":"2025-11-29T23:37:15.318032Z","iopub.status.idle":"2025-11-29T23:44:12.979551Z","shell.execute_reply.started":"2025-11-29T23:37:15.318005Z","shell.execute_reply":"2025-11-29T23:44:12.978717Z"}},"outputs":[{"name":"stdout","text":"Stage-1 training (unconditional GAN) ...\nEpoch 1/20 | D 0.1244 | G 9.1847\nEpoch 2/20 | D 0.2266 | G 6.0280\nEpoch 3/20 | D 0.3328 | G 3.9286\nEpoch 4/20 | D 0.2368 | G 4.1721\nEpoch 5/20 | D 0.2766 | G 3.9763\nEpoch 6/20 | D 0.2445 | G 3.8996\nEpoch 7/20 | D 0.2751 | G 3.6109\nEpoch 8/20 | D 0.3088 | G 3.5989\nEpoch 9/20 | D 0.3236 | G 3.5350\nEpoch 10/20 | D 0.2937 | G 3.7731\nEpoch 11/20 | D 0.2801 | G 3.8494\nEpoch 12/20 | D 0.2296 | G 4.2762\nEpoch 13/20 | D 0.2569 | G 4.1268\nEpoch 14/20 | D 0.2328 | G 4.4283\nEpoch 15/20 | D 0.1525 | G 4.7991\nEpoch 16/20 | D 0.2337 | G 4.4133\nEpoch 17/20 | D 0.2025 | G 4.7696\nEpoch 18/20 | D 0.1842 | G 4.7603\nEpoch 19/20 | D 0.1914 | G 4.8185\nEpoch 20/20 | D 0.1551 | G 5.1287\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 11 — Stage-2: per-class fine-tuning with Freeze-D + lightweight Barlow Twins\n# Implementation notes:\n# - We will copy G and D weights from stage-1 (they already are G and D).\n# - Freeze-D: freeze top n layers (we implement by freezing conv1 & conv2 to match \"freeze highest-resolution\")\n# - Barlow Twins style loss: compute cross-correlation on batch features from two augmented views.\n# - For stability, we will train only for a few epochs per class and use small subsets (paper used full class data, but smaller runs for speed).\n\ndef freeze_D_layers(D_model, freeze_until_layer_idx=2):\n    # freeze first N conv blocks (conv1, conv2, ...)\n    blocks = [D_model.conv1, D_model.conv2, D_model.conv3, D_model.conv4]\n    for i, b in enumerate(blocks):\n        requires = False if i < freeze_until_layer_idx else True\n        for p in b.parameters():\n            p.requires_grad = requires\n    print(f\"Freeze-D: frozen first {freeze_until_layer_idx} blocks\")\n\n# Barlow Twins loss helper\ndef barlow_twins_loss(z_a, z_b, lam=5e-3):\n    # z_a, z_b shape: [B, D]\n    B, D = z_a.size()\n    # normalize (zero mean per dim, unit var)\n    z_a = (z_a - z_a.mean(0)) / (z_a.std(0) + 1e-9)\n    z_b = (z_b - z_b.mean(0)) / (z_b.std(0) + 1e-9)\n    c = (z_a.T @ z_b) / B    # cross-correlation\n    on_diag = torch.diagonal(c).add_(-1).pow(2).sum()\n    off_diag = (c.pow(2).sum() - torch.diagonal(c).pow(2).sum())\n    loss = on_diag + lam * off_diag\n    return loss\n\n# Stage-2 loop parameters\nEPOCHS_STAGE2 = 10   # per class (reduce for Kaggle)\nfreeze_blocks = 2    # freeze first 2 conv blocks (experiment)\n\n# Hyperparams\nlambda_ss = 0.1  # weight for self-supervised loss relative to GAN loss (paper had lambda_ss)\n\n# Iterate classes (you can pick a subset if you want faster)\nclasses_to_run = CLS_NAMES  # you can set e.g., ['mel','nv'] to run fewer\nprint(\"Stage-2 classes:\", classes_to_run)\n\nfor class_name in classes_to_run:\n    paths = class_index.get(class_name, [])\n    if len(paths) < 8:\n        print(f\"Skipping class {class_name} (too few samples: {len(paths)})\")\n        continue\n    print(\"Fine-tuning for class:\", class_name, \"num samples:\", len(paths))\n\n    # build loader — use all class images or a subset for speed\n    subset_paths = paths  # optionally random.sample(paths, min(len(paths), 200))\n    class_dataset = SimpleImageDataset(subset_paths, transform=gan_transform)\n    class_loader = DataLoader(class_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n\n    # Freeze-D\n    freeze_D_layers(D, freeze_until_layer_idx=freeze_blocks)\n\n    # per-class fine-tuning: keep separate optimizers (but here we reuse optimizers; ensure only params requiring_grad present)\n    optD = optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), lr=2.5e-4, betas=(0.5,0.999))\n    optG = optim.Adam(G.parameters(), lr=2.5e-4, betas=(0.5,0.999))\n\n    for epoch in range(EPOCHS_STAGE2):\n        d_loss_running = 0.0\n        g_loss_running = 0.0\n        for i, real in enumerate(class_loader):\n            real = real.to(DEVICE)\n            bs = real.size(0)\n            real_labels = torch.ones(bs,1, device=DEVICE)\n            fake_labels = torch.zeros(bs,1, device=DEVICE)\n\n            # ---------------- D step ----------------\n            optD.zero_grad()\n            # real out + features (we use return_features=True)\n            out_real, feat_real = D(real)\n            z = torch.randn(bs, LATENT_DIM,1,1, device=DEVICE)\n            fake = G(z)\n            out_fake, feat_fake = D(fake.detach())\n\n            loss_d = 0.5*(criterion_bce(out_real, real_labels) + criterion_bce(out_fake, fake_labels))\n            # add Barlow Twins: compute augmented views features from same real batch\n            # create 2 augmented views via bt_aug (PIL transforms)\n            x_a = torch.stack([bt_aug(Image.open(p).convert('RGB')) for p in random.sample(subset_paths, bs)]) if len(subset_paths)>=bs else bt_aug(Image.open(subset_paths[0]).convert('RGB')).unsqueeze(0)\n            # But creating bt views from random sample above complicates mapping; simpler: apply augment on 'real' tensor by re-loading\n            # For simplicity & robustness, compute Barlow Twins on discriminator features of real and fake (two views: real augment + generated augment)\n            # We'll approximate: compute features for two augmentations of the real batch:\n            with torch.no_grad():\n                # reconstruct images from tensor to PIL, apply augment -> re-tensor\n                real_cpu = [transforms.ToPILImage()( (r.cpu()*0.5+0.5).clamp(0,1) ) for r in real]\n            a_views = torch.stack([bt_aug(img) for img in real_cpu]).to(DEVICE)\n            b_views = torch.stack([bt_aug(img) for img in real_cpu]).to(DEVICE)\n            _, feat_a = D(a_views)\n            _, feat_b = D(b_views)\n            loss_bt = barlow_twins_loss(feat_a, feat_b, lam=5e-3)\n            # Combine\n            loss_D = loss_d + lambda_ss * loss_bt\n            loss_D.backward()\n            optD.step()\n\n            # ---------------- G step ----------------\n            optG.zero_grad()\n            z = torch.randn(bs, LATENT_DIM,1,1, device=DEVICE)\n            fake = G(z)\n            out_fake_for_g, feat_fake_for_g = D(fake)\n            loss_gan = criterion_bce(out_fake_for_g, real_labels)\n\n            # also compute a lightweight self-supervised alignment: Barlow between generated features and real features\n            # compute features for generated augment views\n            with torch.no_grad():\n                fake_cpu = [transforms.ToPILImage()( (f.cpu()*0.5+0.5).clamp(0,1) ) for f in fake]\n            a_fake_views = torch.stack([bt_aug(img) for img in fake_cpu]).to(DEVICE)\n            b_fake_views = torch.stack([bt_aug(img) for img in fake_cpu]).to(DEVICE)\n            _, feat_fake_a = D(a_fake_views)\n            _, feat_fake_b = D(b_fake_views)\n            loss_bt_gen = barlow_twins_loss(feat_fake_a, feat_fake_b, lam=5e-3)\n\n            loss_G = loss_gan + lambda_ss * loss_bt_gen * 0.5   # smaller weight for generator BT\n            loss_G.backward()\n            optG.step()\n\n            d_loss_running += loss_D.item()\n            g_loss_running += loss_G.item()\n\n        avg_d = d_loss_running / (i+1)\n        avg_g = g_loss_running / (i+1)\n        print(f\"[{class_name}][Epoch {epoch+1}/{EPOCHS_STAGE2}] D {avg_d:.4f} | G {avg_g:.4f}\")\n\n    # After class fine-tuning, save some examples\n    os.makedirs(f\"/kaggle/working/synth/{class_name}\", exist_ok=True)\n    G.eval()\n    with torch.no_grad():\n        z = torch.randn(25, LATENT_DIM,1,1, device=DEVICE)\n        imgs = G(z)\n        for idx,img in enumerate(imgs):\n            save_image((img+1)/2, f\"/kaggle/working/synth/{class_name}/{class_name}_{idx:03d}.png\")\n    G.train()\n    print(\"Saved synthetic images for class\", class_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:44:12.980687Z","iopub.execute_input":"2025-11-29T23:44:12.981292Z","iopub.status.idle":"2025-11-29T23:58:33.061191Z","shell.execute_reply.started":"2025-11-29T23:44:12.981262Z","shell.execute_reply":"2025-11-29T23:58:33.060378Z"}},"outputs":[{"name":"stdout","text":"Stage-2 classes: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\nFine-tuning for class: akiec num samples: 183\nFreeze-D: frozen first 2 blocks\n[akiec][Epoch 1/10] D 30.9439 | G 16.1852\n[akiec][Epoch 2/10] D 23.8722 | G 13.7313\n[akiec][Epoch 3/10] D 22.2344 | G 14.0105\n[akiec][Epoch 4/10] D 20.9452 | G 13.6140\n[akiec][Epoch 5/10] D 21.3790 | G 14.0787\n[akiec][Epoch 6/10] D 20.5682 | G 13.2922\n[akiec][Epoch 7/10] D 20.1214 | G 12.3527\n[akiec][Epoch 8/10] D 19.4358 | G 13.5350\n[akiec][Epoch 9/10] D 19.1912 | G 13.0068\n[akiec][Epoch 10/10] D 18.8304 | G 13.4582\nSaved synthetic images for class akiec\nFine-tuning for class: bcc num samples: 266\nFreeze-D: frozen first 2 blocks\n[bcc][Epoch 1/10] D 18.3527 | G 15.0512\n[bcc][Epoch 2/10] D 18.9183 | G 14.0357\n[bcc][Epoch 3/10] D 18.4101 | G 13.5439\n[bcc][Epoch 4/10] D 16.9837 | G 14.1138\n[bcc][Epoch 5/10] D 17.2333 | G 13.0958\n[bcc][Epoch 6/10] D 17.1083 | G 12.7602\n[bcc][Epoch 7/10] D 16.9925 | G 12.6168\n[bcc][Epoch 8/10] D 16.7092 | G 12.7496\n[bcc][Epoch 9/10] D 16.3815 | G 11.8075\n[bcc][Epoch 10/10] D 16.5587 | G 11.9760\nSaved synthetic images for class bcc\nFine-tuning for class: bkl num samples: 564\nFreeze-D: frozen first 2 blocks\n[bkl][Epoch 1/10] D 15.4274 | G 11.3116\n[bkl][Epoch 2/10] D 15.0559 | G 11.7376\n[bkl][Epoch 3/10] D 14.6976 | G 11.0964\n[bkl][Epoch 4/10] D 14.1152 | G 10.8421\n[bkl][Epoch 5/10] D 13.7665 | G 10.5558\n[bkl][Epoch 6/10] D 13.7282 | G 11.1905\n[bkl][Epoch 7/10] D 13.5817 | G 9.9630\n[bkl][Epoch 8/10] D 13.4148 | G 10.4849\n[bkl][Epoch 9/10] D 13.2312 | G 10.1067\n[bkl][Epoch 10/10] D 12.9004 | G 9.6027\nSaved synthetic images for class bkl\nFine-tuning for class: df num samples: 56\nFreeze-D: frozen first 2 blocks\n[df][Epoch 1/10] D 14.4671 | G 12.0351\n[df][Epoch 2/10] D 13.4673 | G 10.3409\n[df][Epoch 3/10] D 14.8546 | G 11.5301\n[df][Epoch 4/10] D 15.8549 | G 9.0934\n[df][Epoch 5/10] D 13.9696 | G 11.7290\n[df][Epoch 6/10] D 12.8763 | G 10.8816\n[df][Epoch 7/10] D 13.6685 | G 9.5797\n[df][Epoch 8/10] D 13.6240 | G 11.0330\n[df][Epoch 9/10] D 13.5775 | G 10.7482\n[df][Epoch 10/10] D 13.1055 | G 9.9826\nSaved synthetic images for class df\nFine-tuning for class: mel num samples: 435\nFreeze-D: frozen first 2 blocks\n[mel][Epoch 1/10] D 14.1735 | G 10.1854\n[mel][Epoch 2/10] D 13.4416 | G 9.5071\n[mel][Epoch 3/10] D 13.3543 | G 9.1572\n[mel][Epoch 4/10] D 12.8248 | G 9.3182\n[mel][Epoch 5/10] D 12.6640 | G 8.7926\n[mel][Epoch 6/10] D 12.4411 | G 8.8019\n[mel][Epoch 7/10] D 12.1776 | G 8.6952\n[mel][Epoch 8/10] D 12.0998 | G 8.7853\n[mel][Epoch 9/10] D 12.0883 | G 8.6754\n[mel][Epoch 10/10] D 12.0442 | G 9.0634\nSaved synthetic images for class mel\nFine-tuning for class: nv num samples: 3431\nFreeze-D: frozen first 2 blocks\n[nv][Epoch 1/10] D 13.0485 | G 9.7011\n[nv][Epoch 2/10] D 12.3458 | G 9.5283\n[nv][Epoch 3/10] D 11.8433 | G 9.0789\n[nv][Epoch 4/10] D 11.5260 | G 9.0921\n[nv][Epoch 5/10] D 11.1978 | G 8.8268\n[nv][Epoch 6/10] D 10.9326 | G 8.6255\n[nv][Epoch 7/10] D 10.8477 | G 8.5527\n[nv][Epoch 8/10] D 10.8265 | G 8.3298\n[nv][Epoch 9/10] D 10.3280 | G 8.2708\n[nv][Epoch 10/10] D 10.1376 | G 8.1472\nSaved synthetic images for class nv\nFine-tuning for class: vasc num samples: 65\nFreeze-D: frozen first 2 blocks\n[vasc][Epoch 1/10] D nan | G nan\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_115/2976942131.py:22: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n  z_a = (z_a - z_a.mean(0)) / (z_a.std(0) + 1e-9)\n/tmp/ipykernel_115/2976942131.py:23: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n  z_b = (z_b - z_b.mean(0)) / (z_b.std(0) + 1e-9)\n/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py:282: RuntimeWarning: invalid value encountered in cast\n  npimg = (npimg * 255).astype(np.uint8)\n","output_type":"stream"},{"name":"stdout","text":"[vasc][Epoch 2/10] D nan | G nan\n[vasc][Epoch 3/10] D nan | G nan\n[vasc][Epoch 4/10] D nan | G nan\n[vasc][Epoch 5/10] D nan | G nan\n[vasc][Epoch 6/10] D nan | G nan\n[vasc][Epoch 7/10] D nan | G nan\n[vasc][Epoch 8/10] D nan | G nan\n[vasc][Epoch 9/10] D nan | G nan\n[vasc][Epoch 10/10] D nan | G nan\nSaved synthetic images for class vasc\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 12 — Combine real + synthetic and prepare T-ResNet50 training set\n# Collect generated images into a training folder per class\nSYN_ROOT = Path(\"/kaggle/working/synth\")\naugmented_train_paths = []\naugmented_train_labels = []\n# real training split: create an 80/20 split using metadata if available\nif df_meta is not None:\n    # create train/test split consistent with paper 8:2\n    from sklearn.model_selection import train_test_split\n    ids = df_meta['image_id'].astype(str).tolist()\n    train_ids, val_ids = train_test_split(ids, test_size=0.2, random_state=42, stratify=df_meta['dx'])\n    # build train list paths\n    for tid in train_ids:\n        for ext in ['.jpg','.png','.jpeg']:\n            p = IMAGE_DIR / (str(tid)+ext)\n            if p.exists():\n                lab = df_meta[df_meta['image_id']==tid]['dx'].values[0]\n                augmented_train_paths.append(str(p))\n                augmented_train_labels.append(lab)\n                break\nelse:\n    # fallback: use 80% of full list\n    split = int(0.8 * len(all_image_paths))\n    augmented_train_paths = all_image_paths[:split]\n    augmented_train_labels = ['unknown'] * len(augmented_train_paths)\n\n# add synthetic images (a few per class)\nfor cls in os.listdir(SYN_ROOT) if SYN_ROOT.exists() else []:\n    p = SYN_ROOT / cls\n    imgs = sorted([str(x) for x in p.glob(\"*.png\")])\n    for im in imgs:\n        augmented_train_paths.append(im)\n        augmented_train_labels.append(cls)\n\nprint(\"Total training samples after augmentation:\", len(augmented_train_paths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:58:33.062409Z","iopub.execute_input":"2025-11-29T23:58:33.062826Z","iopub.status.idle":"2025-11-29T23:58:38.968192Z","shell.execute_reply.started":"2025-11-29T23:58:33.062789Z","shell.execute_reply":"2025-11-29T23:58:38.967518Z"}},"outputs":[{"name":"stdout","text":"Total training samples after augmentation: 4185\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 13 — Dataset for classifier training\nclass CldsDataset(Dataset):\n    def __init__(self, paths, labels, transform=None, class_map=None):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        # build class_to_idx\n        if class_map is None:\n            unique = sorted(list(set(labels)))\n            self.class_map = {c:i for i,c in enumerate(unique)}\n        else:\n            self.class_map = class_map\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        img = Image.open(p).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        lbl = self.class_map[self.labels[idx]]\n        return img, lbl\n\n# Train/val split\nfrom sklearn.model_selection import train_test_split\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(augmented_train_paths, augmented_train_labels, test_size=0.2, stratify=augmented_train_labels if len(set(augmented_train_labels))>1 else None, random_state=42)\nclass_map = {c:i for i,c in enumerate(sorted(set(train_labels)))}\ntrain_ds = CldsDataset(train_paths, train_labels, transform=cls_train_transform, class_map=class_map)\nval_ds = CldsDataset(val_paths, val_labels, transform=cls_val_transform, class_map=class_map)\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\nprint(\"Classes map:\", class_map)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:58:38.968986Z","iopub.execute_input":"2025-11-29T23:58:38.969496Z","iopub.status.idle":"2025-11-29T23:58:38.983150Z","shell.execute_reply.started":"2025-11-29T23:58:38.969450Z","shell.execute_reply":"2025-11-29T23:58:38.982277Z"}},"outputs":[{"name":"stdout","text":"Classes map: {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 14 — T-ResNet50 (paper: ResNet50 + FC:2048->128->7)\nnum_classes = len(class_map)\nresnet = models.resnet50(pretrained=True)\n# Replace fc\nresnet.fc = nn.Sequential(\n    nn.Linear(2048,128),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(128, num_classes)\n)\nresnet = resnet.to(DEVICE)\n\n# loss + optimizer + scheduler\ncls_criterion = nn.CrossEntropyLoss()\ncls_optimizer = optim.SGD(resnet.parameters(), lr=0.02, momentum=0.9, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(cls_optimizer, T_max=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:58:38.983976Z","iopub.execute_input":"2025-11-29T23:58:38.984322Z","iopub.status.idle":"2025-11-29T23:58:40.019315Z","shell.execute_reply.started":"2025-11-29T23:58:38.984301Z","shell.execute_reply":"2025-11-29T23:58:40.018641Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 242MB/s]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 15 — Train classifier (fast)\nEPOCHS_CLS = 6\nbest_acc = 0.0\nfor epoch in range(EPOCHS_CLS):\n    # train\n    resnet.train()\n    running_loss = 0.0\n    total = 0\n    correct = 0\n    for images, labels in train_loader:\n        images = images.to(DEVICE); labels = labels.to(DEVICE)\n        cls_optimizer.zero_grad()\n        out = resnet(images)\n        loss = cls_criterion(out, labels)\n        loss.backward()\n        cls_optimizer.step()\n        running_loss += loss.item()\n        preds = out.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    train_acc = correct/total\n    scheduler.step()\n    # val\n    resnet.eval()\n    val_loss = 0.0\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(DEVICE); labels=labels.to(DEVICE)\n            out = resnet(images)\n            loss = cls_criterion(out, labels)\n            val_loss += loss.item()\n            preds = out.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    val_acc = correct/total\n    print(f\"Epoch {epoch+1}/{EPOCHS_CLS} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(resnet.state_dict(), \"/kaggle/working/t_resnet50_best.pth\")\nprint(\"Best val acc:\", best_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T23:58:40.020184Z","iopub.execute_input":"2025-11-29T23:58:40.020487Z","iopub.status.idle":"2025-11-30T00:01:06.874391Z","shell.execute_reply.started":"2025-11-29T23:58:40.020445Z","shell.execute_reply":"2025-11-30T00:01:06.873258Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/6 | Train Acc: 0.6738 | Val Acc: 0.7384\nEpoch 2/6 | Train Acc: 0.7222 | Val Acc: 0.7097\nEpoch 3/6 | Train Acc: 0.7458 | Val Acc: 0.7431\nEpoch 4/6 | Train Acc: 0.7754 | Val Acc: 0.7348\nEpoch 5/6 | Train Acc: 0.7855 | Val Acc: 0.7599\nEpoch 6/6 | Train Acc: 0.8100 | Val Acc: 0.7718\nBest val acc: 0.7718040621266428\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Cell 16 — Optional: Test-time augmentation (TTA) inference for val set\n# example TTA: horizontal flip and center crop (average probs)\ntta_transforms = [\n    cls_val_transform,\n    transforms.Compose([transforms.Resize((IMG_SIZE_CLS, IMG_SIZE_CLS)), transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor(),\n                       transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n]\n\ndef tta_predict(model, pil_img, tta_transforms):\n    model.eval()\n    probs = []\n    with torch.no_grad():\n        for t in tta_transforms:\n            x = t(pil_img).unsqueeze(0).to(DEVICE)\n            out = model(x)\n            probs.append(torch.softmax(out, dim=1))\n    return torch.mean(torch.stack(probs), dim=0)\n\n# quick TTA eval on val set (first 50 images)\nmodel = resnet\ncorrect = 0; total = 0\nfor i, (p,l) in enumerate(zip(val_paths[:50], val_labels[:50])):\n    pil = Image.open(p).convert('RGB')\n    prob = tta_predict(model, pil, tta_transforms)\n    pred = prob.argmax(1).item()\n    if pred == class_map[l]: correct += 1\n    total += 1\nprint(\"TTA quick accuracy (first 50 val):\", correct/total)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:01:06.875917Z","iopub.execute_input":"2025-11-30T00:01:06.876305Z","iopub.status.idle":"2025-11-30T00:01:08.352594Z","shell.execute_reply.started":"2025-11-30T00:01:06.876262Z","shell.execute_reply":"2025-11-30T00:01:08.351439Z"}},"outputs":[{"name":"stdout","text":"TTA quick accuracy (first 50 val): 0.8\n","output_type":"stream"}],"execution_count":18}]}